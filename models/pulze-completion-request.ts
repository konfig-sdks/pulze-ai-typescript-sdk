/*
Pulze.ai API

At Pulze it's our mission to supercharge today's workforce with AI to maximize the world's prosperity. We are doing so by enabling companies of any size to securely leverage Large Language Models (LLM) and easily build AI features into their apps. Our enterprise platform has access to all best in class LLMs and can route user requests to the most relevant model to get the highest quality response at the best price thanks to our smart meta model. End users can leverage pre-built applications, such as our Marketing AI product, or build custom apps on top of the Pulze Platform.

We are a VC Funded, early stage startup based in San Francisco.

The version of the OpenAPI document: 0.1.0


NOTE: This file is auto generated by Konfig (https://konfigthis.com).
*/
import type * as buffer from "buffer"

import { LLMModelPolicies } from './llmmodel-policies';
import { LLMModelWeights } from './llmmodel-weights';
import { PromptProperty } from './prompt-property';
import { RoleContentChatChoice } from './role-content-chat-choice';
import { StopProperty } from './stop-property';
import { ToolChoiceProperty } from './tool-choice-property';
import { ToolFunction } from './tool-function';

/**
 * 
 * @export
 * @interface PulzeCompletionRequest
 */
export interface PulzeCompletionRequest {
    /**
     * The number of responses to _generate_. Out of those, it will return the best `n`.
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'best_of'?: number;
    /**
     * The maximum context size (tokens) for the provided model
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'context_window'?: number;
    /**
     * How many completions to generate for each prompt. @default 1 
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'false'?: number;
    /**
     * https://platform.openai.com/docs/api-reference/completions/create#completions/create-frequency_penalty Increase the model\'s likelihood to not repeat tokens/words 
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'frequency_penalty'?: number;
    /**
     * Headers sent alongside the request -- stored as part of the payload
     * @type {{ [key: string]: string; }}
     * @memberof PulzeCompletionRequest
     */
    'headers'?: { [key: string]: string; };
    /**
     * The labels sent alongside the request as a JSON-encoded Dict[str, str] inside of the header \"pulze-labels\".
     * @type {{ [key: string]: string; }}
     * @memberof PulzeCompletionRequest
     */
    'labels'?: { [key: string]: string; };
    /**
     * COMING SOON https://platform.openai.com/docs/api-reference/completions/create#completions/create-logit_bias Modify the likelihood of specified tokens appearing in the completion.  See here for a detailed explanation on how to use: https://help.openai.com/en/articles/5247780-using-logit-bias-to-define-token-probability 
     * @type {object}
     * @memberof PulzeCompletionRequest
     */
    'logit_bias'?: object;
    /**
     * COMING SOON https://platform.openai.com/docs/api-reference/completions/create#completions/create-logprobs Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. 
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'logprobs'?: number;
    /**
     * The maximum number of tokens that the response can contain.
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'max_tokens'?: number;
    /**
     * The conversation sent (with or without history) (for a /chat/completions request)
     * @type {Array<RoleContentChatChoice>}
     * @memberof PulzeCompletionRequest
     */
    'messages'?: Array<RoleContentChatChoice>;
    /**
     * https://docs.pulze.ai/overview/models Specify the model you\'d like Pulze to use. (optional). Can be the full model name, or a subset for multi-matching.  Defaults to our dynamic routing, i.e. best model for this request. 
     * @type {string}
     * @memberof PulzeCompletionRequest
     */
    'model'?: string;
    /**
     * Settings for how this request should be processed: Anonimized, public, and more.
     * @type {LLMModelPolicies}
     * @memberof PulzeCompletionRequest
     */
    'policies'?: LLMModelPolicies;
    /**
     * https://platform.openai.com/docs/api-reference/completions/create#completions/create-presence_penalty Increase the model\'s likelihood to talk about new topics 
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'presence_penalty'?: number;
    /**
     * 
     * @type {PromptProperty}
     * @memberof PulzeCompletionRequest
     */
    'prompt'?: PromptProperty;
    /**
     * The name of the provider for the request
     * @type {string}
     * @memberof PulzeCompletionRequest
     */
    'provider'?: string;
    /**
     * 
     * @type {StopProperty}
     * @memberof PulzeCompletionRequest
     */
    'stop'?: StopProperty;
    /**
     * ** COMING SOON ** Specify if you want the response to be streamed or to be returned as a standard HTTP request 
     * @type {boolean}
     * @memberof PulzeCompletionRequest
     */
    'stream'?: boolean;
    /**
     * COMING SOON
     * @type {string}
     * @memberof PulzeCompletionRequest
     */
    'suffix'?: string;
    /**
     * Optionally specify the temperature for this request only. Leave empty to allow Pulze to guess it for you.
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'temperature'?: number;
    /**
     * 
     * @type {ToolChoiceProperty}
     * @memberof PulzeCompletionRequest
     */
    'tool_choice'?: ToolChoiceProperty;
    /**
     * 
     * @type {Array<ToolFunction>}
     * @memberof PulzeCompletionRequest
     */
    'tools'?: Array<ToolFunction>;
    /**
     * https://platform.openai.com/docs/api-reference/completions/create#completions/create-top_p An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass 
     * @type {number}
     * @memberof PulzeCompletionRequest
     */
    'top_p'?: number;
    /**
     * Optionally specify specific weights for this request only. Leave empty to use the App\'s weights (global configuration)
     * @type {LLMModelWeights}
     * @memberof PulzeCompletionRequest
     */
    'weights'?: LLMModelWeights;
}

