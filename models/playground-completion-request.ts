/*
Pulze.ai API

At Pulze it's our mission to supercharge today's workforce with AI to maximize the world's prosperity. We are doing so by enabling companies of any size to securely leverage Large Language Models (LLM) and easily build AI features into their apps. Our enterprise platform has access to all best in class LLMs and can route user requests to the most relevant model to get the highest quality response at the best price thanks to our smart meta model. End users can leverage pre-built applications, such as our Marketing AI product, or build custom apps on top of the Pulze Platform.

We are a VC Funded, early stage startup based in San Francisco.

The version of the OpenAPI document: 0.1.0


NOTE: This file is auto generated by Konfig (https://konfigthis.com).
*/
import type * as buffer from "buffer"

import { LLMModelWeights } from './llmmodel-weights';
import { RoleContentChatChoice } from './role-content-chat-choice';

/**
 * 
 * @export
 * @interface PlaygroundCompletionRequest
 */
export interface PlaygroundCompletionRequest {
    /**
     * Optionally, send an App ID belonging to this user\'s org and all the logs will be logged into that app. This usually means the user is using the playground from the app\'s page itself.
     * @type {string}
     * @memberof PlaygroundCompletionRequest
     */
    'app_id'?: string;
    /**
     * The maximum number of tokens for the request
     * @type {number}
     * @memberof PlaygroundCompletionRequest
     */
    'max_tokens'?: number;
    /**
     * The list of messages (user/assistant/user/...) for the prompt. At least one message required
     * @type {Array<RoleContentChatChoice>}
     * @memberof PlaygroundCompletionRequest
     */
    'messages'?: Array<RoleContentChatChoice>;
    /**
     * An optional model name. If specified, that model will be used
     * @type {string}
     * @memberof PlaygroundCompletionRequest
     */
    'model'?: string;
    /**
     * The temperature of the request
     * @type {number}
     * @memberof PlaygroundCompletionRequest
     */
    'temperature'?: number;
    /**
     * @required The weights specific to this request
     * @type {LLMModelWeights}
     * @memberof PlaygroundCompletionRequest
     */
    'weights'?: LLMModelWeights;
}

