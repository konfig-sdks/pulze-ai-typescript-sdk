/*
Pulze.ai API

At Pulze it's our mission to supercharge today's workforce with AI to maximize the world's prosperity. We are doing so by enabling companies of any size to securely leverage Large Language Models (LLM) and easily build AI features into their apps. Our enterprise platform has access to all best in class LLMs and can route user requests to the most relevant model to get the highest quality response at the best price thanks to our smart meta model. End users can leverage pre-built applications, such as our Marketing AI product, or build custom apps on top of the Pulze Platform.

We are a VC Funded, early stage startup based in San Francisco.

The version of the OpenAPI document: 0.1.0


NOTE: This file is auto generated by Konfig (https://konfigthis.com).
*/
import type * as buffer from "buffer"


/**
 * 
 * @export
 * @interface LLMModelPolicies
 */
export interface LLMModelPolicies {
    /**
     * The maximum cost allowed for a request. Only works with compounded requests that require multiple LLM calls. If the value is reached, it will exit with an exception.
     * @type {number}
     * @memberof LLMModelPolicies
     */
    'max_cost'?: number;
    /**
     * If an LLM call fails, how many times should Pulze _retry the call to the same LLM_? There will be a maximum of N+1 calls (original + N retries)
     * @type {number}
     * @memberof LLMModelPolicies
     */
    'max_same_model_retries'?: number;
    /**
     * If an LLM call fails, _how many other models_ should Pulze try, chosen by quality descending? It will be a maximum of N+1 models (original + N other models)
     * @type {number}
     * @memberof LLMModelPolicies
     */
    'max_switch_model_retries'?: number;
    /**
     * Optimize the internal / intermediate LLM requests, for a big gain in speed and cost savings, at the cost of a potential, and very slight, penalty on quality. The final request (\"SYNTHESIZE\") is always performed using your original settings.
     * @type {number}
     * @memberof LLMModelPolicies
     */
    'optimize_internal_requests'?: LLMModelPoliciesOptimizeInternalRequestsEnum;
    /**
     *          The level of privacy for a given request         0 = (UNSUPPORTED -- public logs)         1 = Log request, response and all of its metadata (Normal mode)         2 = Do not log neither the request prompt nor the response text. Logs are still visible, and all of the request metadata accessible. Retrievable as a log. (TBD)         3 = Do not log at all. Internally, a minimal representation may be stored for billing: model name, tokens used, which app it belongs to, and timestamp. Not retrievable as a log. (TBD)         
     * @type {number}
     * @memberof LLMModelPolicies
     */
    'privacy_level'?: LLMModelPoliciesPrivacyLevelEnum;
    /**
     * Prompt ID that we will use for requests
     * @type {string}
     * @memberof LLMModelPolicies
     */
    'prompt_id'?: string;
}

type LLMModelPoliciesOptimizeInternalRequestsEnum = 0 | 1
type LLMModelPoliciesPrivacyLevelEnum = 1 | 2 | 3


