/*
Pulze.ai API

At Pulze it's our mission to supercharge today's workforce with AI to maximize the world's prosperity. We are doing so by enabling companies of any size to securely leverage Large Language Models (LLM) and easily build AI features into their apps. Our enterprise platform has access to all best in class LLMs and can route user requests to the most relevant model to get the highest quality response at the best price thanks to our smart meta model. End users can leverage pre-built applications, such as our Marketing AI product, or build custom apps on top of the Pulze Platform.

We are a VC Funded, early stage startup based in San Francisco.

The version of the OpenAPI document: 0.1.0


NOTE: This file is auto generated by Konfig (https://konfigthis.com).
*/
import type * as buffer from "buffer"

import { PulzeCompletionRequest } from './pulze-completion-request';
import { PulzeEngineResponse } from './pulze-engine-response';
import { RequestInDBBase } from './request-in-dbbase';

/**
 * 
 * @export
 * @interface Request
 */
export interface Request {
    /**
     * The ID of the app that performed the request
     * @type {string}
     * @memberof Request
     */
    'app_id'?: string;
    /**
     * The children of the Request. Will equal None unless you use eager loading in the query
     * @type {Array<RequestInDBBase>}
     * @memberof Request
     */
    'children'?: Array<RequestInDBBase>;
    /**
     * When a request requires multiple intermediate calls, they are stored as \'no costs incurred\' -- that way we can store the costs, but don\'t charge the user
     * @type {boolean}
     * @memberof Request
     */
    'costs_incurred'?: boolean;
    /**
     * A free text providing more detailed feedback
     * @type {string}
     * @memberof Request
     */
    'feedback'?: string;
    /**
     * The rating for the request
     * @type {boolean}
     * @memberof Request
     */
    'good_answer'?: boolean;
    /**
     * ID of the request
     * @type {string}
     * @memberof Request
     */
    'id': string;
    /**
     * The name of the provider\'s model which was used to answer the request
     * @type {string}
     * @memberof Request
     */
    'namespace'?: string;
    /**
     * The parent of the Request, if any. Requests which are part of a series of sub-requests (like multiple LLM calls, or RAG) will have the final, resulting Log as parent.
     * @type {RequestInDBBase}
     * @memberof Request
     */
    'parent'?: RequestInDBBase;
    /**
     * Reference to the ID of the parent of this log. A log has a parent when it\'s a subrequest used to retrieve the final answer.
     * @type {string}
     * @memberof Request
     */
    'parent_id'?: string;
    /**
     * The payload sent with the request
     * @type {PulzeCompletionRequest}
     * @memberof Request
     */
    'payload'?: PulzeCompletionRequest;
    /**
     * How much is logged? 1: everything, 2: mask request+response (but show log), 3: Not visible, not retrievable, no information stored.
     * @type {number}
     * @memberof Request
     */
    'privacy_level'?: RequestPrivacyLevelEnum;
    /**
     * The prompt in text format
     * @type {string}
     * @memberof Request
     */
    'prompt'?: string;
    /**
     * The type of request (text completion or chat) the user sends and expects back
     * @type {string}
     * @memberof Request
     */
    'request_type'?: RequestRequestTypeEnum;
    /**
     * The response object
     * @type {PulzeEngineResponse}
     * @memberof Request
     */
    'response': PulzeEngineResponse;
    /**
     * The response in text format
     * @type {string}
     * @memberof Request
     */
    'response_text'?: string;
    /**
     * The status code of the request to the AI model
     * @type {number}
     * @memberof Request
     */
    'status_code'?: number;
}

type RequestPrivacyLevelEnum = 1 | 2 | 3
type RequestRequestTypeEnum = 'completions' | 'chat_completions'


